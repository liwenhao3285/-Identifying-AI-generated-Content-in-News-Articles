{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\lihon\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Dropout, Conv1D, MaxPool1D, GlobalMaxPool1D, Embedding, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import PorterStemmer\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = data['text']\n",
    "y = data['classes']\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(x,y,random_state=1,test_size=0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text data\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = ''.join([word for word in text if word not in string.punctuation])\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "X_train = X_train.apply(preprocess)\n",
    "X_test = X_test.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Word2Vec model\n",
    "sentences = [sentence.split() for sentence in X_train]\n",
    "w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = Tokenizer(7229)\n",
    "token.fit_on_texts(data['text'])\n",
    "text = token.texts_to_sequences(data['text'])\n",
    "text = pad_sequences(text, 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(data['classes'])\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(np.array(text),y,random_state=1,test_size=0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gensim_to_keras_embedding(model, train_embeddings=False):\n",
    "    \"\"\"Get a Keras 'Embedding' layer with weights set from Word2Vec model's learned word embeddings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_embeddings : bool\n",
    "        If False, the returned weights are frozen and stopped from being updated.\n",
    "        If True, the weights can / will be further updated in Keras.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    `keras.layers.Embedding`\n",
    "        Embedding layer, to be used as input to deeper network layers.\n",
    "\n",
    "    \"\"\"\n",
    "    keyed_vectors = model.wv  # structure holding the result of training\n",
    "    weights = keyed_vectors.vectors  # vectors themselves, a 2D numpy array    \n",
    "    index_to_key = keyed_vectors.index_to_key  # which row in `weights` corresponds to which word?\n",
    "\n",
    "    layer = Embedding(\n",
    "        input_dim=weights.shape[0],\n",
    "        output_dim=weights.shape[1],\n",
    "        weights=[weights],\n",
    "        trainable=train_embeddings,\n",
    "    )\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "3200/3200 [==============================] - 17s 5ms/step - loss: 0.4363 - acc: 0.8070 - val_loss: 0.2758 - val_acc: 0.8691\n",
      "Epoch 2/30\n",
      "3200/3200 [==============================] - 15s 5ms/step - loss: 0.2958 - acc: 0.8617 - val_loss: 0.2552 - val_acc: 0.8803\n",
      "Epoch 3/30\n",
      "3200/3200 [==============================] - 15s 5ms/step - loss: 0.2774 - acc: 0.8729 - val_loss: 0.2465 - val_acc: 0.8838\n",
      "Epoch 4/30\n",
      "3200/3200 [==============================] - 15s 5ms/step - loss: 0.2608 - acc: 0.8823 - val_loss: 0.2441 - val_acc: 0.8898\n",
      "Epoch 5/30\n",
      "3200/3200 [==============================] - 16s 5ms/step - loss: 0.2524 - acc: 0.8864 - val_loss: 0.2789 - val_acc: 0.8752\n",
      "Epoch 6/30\n",
      "3200/3200 [==============================] - 16s 5ms/step - loss: 0.2450 - acc: 0.8913 - val_loss: 0.2888 - val_acc: 0.8798\n",
      "Epoch 7/30\n",
      "3200/3200 [==============================] - 16s 5ms/step - loss: 0.2383 - acc: 0.8969 - val_loss: 0.2575 - val_acc: 0.8870\n",
      "Epoch 8/30\n",
      "3200/3200 [==============================] - 16s 5ms/step - loss: 0.2286 - acc: 0.9022 - val_loss: 0.2293 - val_acc: 0.8974\n",
      "Epoch 9/30\n",
      "3200/3200 [==============================] - 16s 5ms/step - loss: 0.2245 - acc: 0.9045 - val_loss: 0.2331 - val_acc: 0.8975\n",
      "Epoch 10/30\n",
      "3200/3200 [==============================] - 16s 5ms/step - loss: 0.2190 - acc: 0.9063 - val_loss: 0.2314 - val_acc: 0.8980\n",
      "Epoch 11/30\n",
      "3200/3200 [==============================] - 16s 5ms/step - loss: 0.2156 - acc: 0.9089 - val_loss: 0.2280 - val_acc: 0.8991\n",
      "Epoch 12/30\n",
      "3200/3200 [==============================] - 16s 5ms/step - loss: 0.2104 - acc: 0.9120 - val_loss: 0.2336 - val_acc: 0.8995\n",
      "Epoch 13/30\n",
      "3200/3200 [==============================] - 16s 5ms/step - loss: 0.2062 - acc: 0.9138 - val_loss: 0.2540 - val_acc: 0.8910\n",
      "Epoch 14/30\n",
      "3200/3200 [==============================] - 16s 5ms/step - loss: 0.2024 - acc: 0.9159 - val_loss: 0.2812 - val_acc: 0.8844\n",
      "Epoch 15/30\n",
      "3200/3200 [==============================] - 16s 5ms/step - loss: 0.2040 - acc: 0.9153 - val_loss: 0.2375 - val_acc: 0.8981\n",
      "Epoch 16/30\n",
      "3200/3200 [==============================] - 16s 5ms/step - loss: 0.1996 - acc: 0.9172 - val_loss: 0.2787 - val_acc: 0.8896\n",
      "Epoch 17/30\n",
      "3200/3200 [==============================] - 16s 5ms/step - loss: 0.2019 - acc: 0.9158 - val_loss: 0.2518 - val_acc: 0.9012\n",
      "Epoch 18/30\n",
      "3200/3200 [==============================] - 16s 5ms/step - loss: 0.1955 - acc: 0.9199 - val_loss: 0.2554 - val_acc: 0.8889\n",
      "Epoch 19/30\n",
      "3200/3200 [==============================] - 16s 5ms/step - loss: 0.1928 - acc: 0.9200 - val_loss: 0.4366 - val_acc: 0.8732\n",
      "Epoch 20/30\n",
      "3200/3200 [==============================] - 16s 5ms/step - loss: 0.1934 - acc: 0.9201 - val_loss: 0.2453 - val_acc: 0.8959\n",
      "Epoch 21/30\n",
      "3200/3200 [==============================] - 16s 5ms/step - loss: 0.1934 - acc: 0.9201 - val_loss: 0.2451 - val_acc: 0.8955\n",
      "Epoch 22/30\n",
      "3200/3200 [==============================] - 16s 5ms/step - loss: 0.1860 - acc: 0.9242 - val_loss: 0.3176 - val_acc: 0.8918\n",
      "Epoch 23/30\n",
      "3200/3200 [==============================] - 16s 5ms/step - loss: 0.1891 - acc: 0.9239 - val_loss: 0.2582 - val_acc: 0.8859\n",
      "Epoch 24/30\n",
      "3200/3200 [==============================] - 16s 5ms/step - loss: 0.1842 - acc: 0.9251 - val_loss: 0.3714 - val_acc: 0.8894\n",
      "Epoch 25/30\n",
      "3200/3200 [==============================] - 16s 5ms/step - loss: 0.1877 - acc: 0.9238 - val_loss: 0.2806 - val_acc: 0.8985\n",
      "Epoch 26/30\n",
      "3200/3200 [==============================] - 16s 5ms/step - loss: 0.1817 - acc: 0.9269 - val_loss: 0.3382 - val_acc: 0.8799\n",
      "Epoch 27/30\n",
      "3200/3200 [==============================] - 16s 5ms/step - loss: 0.1833 - acc: 0.9265 - val_loss: 0.2772 - val_acc: 0.9028\n",
      "Epoch 28/30\n",
      "3200/3200 [==============================] - 16s 5ms/step - loss: 0.1808 - acc: 0.9279 - val_loss: 0.2340 - val_acc: 0.9019\n",
      "Epoch 29/30\n",
      "3200/3200 [==============================] - 16s 5ms/step - loss: 0.1805 - acc: 0.9285 - val_loss: 0.2431 - val_acc: 0.9002\n",
      "Epoch 30/30\n",
      "3200/3200 [==============================] - 16s 5ms/step - loss: 0.1810 - acc: 0.9285 - val_loss: 0.2370 - val_acc: 0.8948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x217e3bce550>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_model1 = Sequential()\n",
    "def build_model(model):\n",
    "    model.add(gensim_to_keras_embedding(w2v_model))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(300, 3, activation='relu', padding='same', strides=1))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dense(200))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='binary_crossentropy', metrics=['acc'], optimizer='adam')\n",
    "\n",
    "build_model(keras_model1)\n",
    "keras_model1.fit(X_train, y_train, batch_size=16, epochs=30, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 1s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90      6528\n",
      "           1       0.89      0.89      0.89      6272\n",
      "\n",
      "    accuracy                           0.89     12800\n",
      "   macro avg       0.89      0.89      0.89     12800\n",
      "weighted avg       0.89      0.89      0.89     12800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_predict = keras_model1.predict(X_test)\n",
    "predicted_categories = np.argmax(y_predict, axis=1)\n",
    "\n",
    "true_categories_argmax = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(classification_report(true_categories_argmax, predicted_categories))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
